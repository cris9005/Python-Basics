{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb1f1f9-09c2-4a45-85b0-3a8e6d65231b",
   "metadata": {},
   "source": [
    "## Notebook with basics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd3c59-254b-4986-89a9-54ac0d2f515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data given\n",
    "\n",
    "customer = ['customer_id', 'birth_date', 'acquisition_channel']\n",
    "order = ['order_id', 'transaction_date', 'product', 'price', 'quantity', 'customer_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6619f7e7-01c5-4f14-9911-25a862d96224",
   "metadata": {},
   "source": [
    "## Review Data\n",
    "\n",
    "1. Make sure dates are in correct format \n",
    "2. Remove NA values\n",
    "3. Calculate the sales17 and sales 18 by filtering date by years\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35422386-e76c-4e4e-96eb-78ac4e4c6fd9",
   "metadata": {},
   "source": [
    "##  Q1 Write a function to calculate the relative difference between total sales in 2017-2018\n",
    " - relative difference = (sales2018/sales2017) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f37382-81d5-4fbb-95b0-1fc407c87c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sale17 = order['transaction_date' == 2017]\n",
    "sale17.price.sum * sale17.quantity.sum\n",
    "\n",
    "df = pd.read_csv('data/data_4.csv',\n",
    "                 parse_dates={ 'date': ['year', 'month', 'day'] })\n",
    "df.info()\n",
    "\n",
    "df = pd.read_csv('data/data_3.csv', parse_dates=['date'])\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21a824-4b06-4990-bc77-d5e55357a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Total Price\"] = df[\"Units\"] * df[\"Unit Price\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d3e654-1bd5-4503-bb0a-8f2b0880f7ab",
   "metadata": {},
   "source": [
    "## Q2 Write a function to return median age per acquistion channel.\n",
    " should be a pandas dataframe wtih two columns 'acquistion_channel' and 'median_age' with respectative values\n",
    " \n",
    " groupby acquistion channel and then calculate the age\n",
    " convert the age column into interger type before calculate the median age. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfce643-d30f-4e3f-911e-20828534dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(airport_data.groupby(['acquistion_channel'])['Departure_Delay_Minutes'].mean())\n",
    "\n",
    "# Use Python's built-in datetime module\n",
    "def calculateAge(birthDate):\n",
    "    today = date.today()\n",
    "    age = today.year - birthDate.year - ((today.month, today.day) < (birthDate.month, birthDate.day))\n",
    "    return age\n",
    "(pd.to_datetime('today').year-pd.to_datetime('1956-07-01').year)\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#convert columns to datetime\n",
    "df[['start_date','end_date']] = df[['start_date','end_date']].apply(pd.to_datetime)\n",
    "\n",
    "#calculate difference between dates\n",
    "df['diff_days'] = (df['end_date'] - df['start_date']) / np.timedelta64(1, 'D')\n",
    "\n",
    "#view updated DataFrame\n",
    "print(df)\n",
    "\n",
    "df[\"value\"] = df.groupby(\"name\").transform(lambda x: x.fillna(x.mean()))\n",
    " df['value1']=df.groupby('name')['value'].apply(lambda x:x.fillna(x.mean()))\n",
    ">>> df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2738a-1e29-4e5e-8cfb-949ce3347cbc",
   "metadata": {},
   "source": [
    "## Q3 Write a function to return the most popular product in terms of units sold amoung people born between 1990 and 2000\n",
    "- had to use both tables, perform a join on customer_id.\n",
    "filter rows by age\n",
    "group by products and calculate the sum of quantity\n",
    "return the most frequent aka maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fd4f5-6bff-4b61-afdf-d1e5c3921700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10597783-5ff4-463e-9420-97fe3fb97c08",
   "metadata": {},
   "source": [
    "## Q4 and Q5: \n",
    "The marketing department wants to send personalized promotions to targeted customers. \n",
    "They want to know about customers who will generate the most amount of revenue.\n",
    "For this, you need to create a regression model that predicts demand for a given customer and year. \n",
    " - target variable is revenue = price * quanitity aggregated at yearly level\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b3f1b-febd-4f4c-8d48-78305eb032f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Q4 write a function that does the following:\n",
    "1.  using order and customer datasets create an aggregated table of revenues by year and customer_id\n",
    "2. add the following features into the aggregated table as well:\n",
    " - age: customer age, previous_year_products: # of distinct products brought by the customer in previous year, previous_year_spend: rev. generated by customer in previous year\n",
    "3. Add at least two more features from your side to improve the model\n",
    "4. split the data into test/train sets\n",
    "        test: orders in 2018\n",
    "        train: orders from all other years\n",
    "5. Calculate the RMSE on the test and return it to the function\n",
    " \n",
    "what is rev (price*quantity) by year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e587f4-e259-47b4-a854-738019012890",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = year, revenue, customer_id, customer age, pre_year_products for each customer, prev_year rev \n",
    "\n",
    "features_selected = housing_data[housing_values].corr()['SalePrice'].sort_values(ascending=False)[1:10].index\n",
    "\n",
    "features_selected = list(features_selected)\n",
    "features_selected\n",
    "# My selection of features that have a higher correlation with SalesPrice. \n",
    "\n",
    "\n",
    "This measures the overall accuracy of the model and is a basis for comparing it to \n",
    "other models (including models fit using machine learning techniques). \n",
    "Similar to RMSE is the residual standard error, or RSE. In this case we have p predictors, and the RSE is given by:\n",
    "\n",
    "fitted = house_lm.predict(house[predictors])\n",
    "RMSE = np.sqrt(mean_squared_error(house[outcome], fitted))\n",
    "r2 = r2_score(house[outcome], fitted)\n",
    "print(f'RMSE: {RMSE:.0f}')\n",
    "print(f'r2: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651d1cf-8866-4633-bd51-fd93bdfabef2",
   "metadata": {},
   "source": [
    "# Q5 In the data provided, only quantity of sold products is present. \n",
    "What important data/features are missing that can help you get closer to the real demand?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972998f-4afb-4de0-b0c4-7388b9c31c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Think about what other features? education of customers, gender of the customers, ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10d6bc-bf06-465a-a23b-3dc9dac44ace",
   "metadata": {},
   "source": [
    "## SECTION 2: 2 Questions\n",
    "\n",
    "OLS Regression summary results were given with variables, coefficients and p-value:\n",
    "    \n",
    "    variable| coefficients | p-value\n",
    "    TV | 11000 | 0\n",
    "    radio | 6000 | 0.005\n",
    "    website | 9000 | 0    \n",
    "    billboard | 6000 | 0.0005\n",
    "    constant | 40000 | 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732239b-a903-470b-b417-ea57c3a244ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 Which variable is the most effective marketing channel?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67faa493-bec5-4e67-86b1-6cf4d05bdd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 Which variable is the least effective marketing channel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14231802-e779-4484-be49-52fb56ac3d27",
   "metadata": {},
   "source": [
    "## SECTION 3: 1 QUESTION\\\n",
    "The following information is given with respect to \"cancer\":\n",
    "    -1% of the total population has it\n",
    "    -you built a binary classification model to predict if a person has Cancer.\n",
    "    1 means the person will have cancer and 0 means the person will not have cancer\n",
    "    - for people with cancer, your model is correct 99% of the time\n",
    "    - for rest of the people, your model is 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b30e9-8687-43c2-a9bf-2694a1ec0e6f",
   "metadata": {},
   "source": [
    "## Q8 You run your model on a person and the model predicted 1.\n",
    "what is the probability that this person has Cancer?\n",
    "\n",
    "Need to review base thoerm. Classic example of baynes theorm. Muliple choice. 8 or 9 and only 1 is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f350ac-6403-4f16-b752-c9d951456aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03187cbd-882d-441f-8add-5f930a99b45d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SECTION 4: 1 Question\n",
    "Lets suppose you have to develop a regression model. After gathering all the data and building all the features, you end up with 10000 records and 12000 features.\n",
    "#Q9 At this moment, \n",
    "1. what will be you next step?\n",
    "2. what different techniques are available for your next step?\n",
    "3. what are the key differences between those techniquies?\n",
    "\n",
    "-- alot of features but not alot of records\n",
    "dimensioyualy reductions. not what step is next? EDA, visualizations, key differences between techniques. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff6f1f-2b69-4b95-859a-be06ea17fcb3",
   "metadata": {},
   "source": [
    "Python Sample question looks like this and code following.\n",
    "\n",
    "Fill NA values with mean groupby ‘Department’;\n",
    "\n",
    "Given following list [‘aQx 12’, ‘aub 6 5’], get the character values only.\n",
    "\n",
    "Scale minmax for the following random array(random seed is 42)\n",
    "\n",
    "add 10% of the value of the original array using np.vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36def098-f441-4d1e-a272-d1e4b0ad252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining datafiles and resetting the index\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "#Merge data on same common column\n",
    "df_air - pd.merge(df1, df2, on ='sharedcolumn')\n",
    "\n",
    "## create new df\n",
    "df_new = df[['col1', 'col2']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "df['Transaction_date'] = pd.to_datetime(df['Transaction_date'], format ='%Y-%m').dt.strftime('%Y-%m)\n",
    "                                                                                             \n",
    "                                                                                             \n",
    "# Filter by years\n",
    "df17 = df[(df['date'] >= '2018-01') & (df['date'] <='2019-12') &\n",
    "(df['arr_flights'}.notnull())\n",
    "    & (df['col2'].notnull()) & (df['col3'].notnull())\n",
    "    & (df['col4'].notnull()) & (df['col5'].notnull())]\n",
    " \n",
    " \n",
    " ## Filter by  specific value in column\n",
    " df['newcol1'] = df['colw/value'].apply(lambda x: x.find('newcol1'))\n",
    " \n",
    " #creates list\n",
    " newcols = set(df[df['newcol1'] != -1]['colw/value'])\n",
    " \n",
    " \n",
    " ## powerful pandas function CROSS TABULATION \n",
    " pd.crosstab(df['co1'], df['col2'], values = df['colofinterest'], aggfun='sum').fillna('')\n",
    " \n",
    " \n",
    " ## how many of X into Z in 2019 experienced A and B?\n",
    " df_f = df[(df['date'] >='2019-01') & (df['date'] <='2019-12') &\n",
    "           (df['col2'] == 'X') & (df['A'] > 0) & (df['B'] > 0)]\n",
    " \n",
    " print(\"Num:\" + str(df_f['A'].sum() + df_f['B'].sum())\n",
    "       \n",
    "## useful code for equcations\n",
    "      \n",
    "       \n",
    "dfprice = df.groupby('product')['price'].sum().reset_index(name = 'price_prod')\n",
    "dfquantity = df.groupby('product')['quantity'].sum().reset_index(name = 'prod_sold')\n",
    "df17sales = pd.merge(dfprice, dfquantity, on = 'product')\n",
    "       \n",
    "df17sales['sales'] = df17sales['price_prod'] * df17sales['prod_sold']\n",
    "       \n",
    "## avg delays per Z\n",
    "avg_delays = df.groupby('Z')['delays'].sum().mean()\n",
    "       \n",
    "       \n",
    "## display the 4 carriers with the smallest Y\n",
    "df.groupby('carriers')['Y'].sum().nsmallest(4).reset_index(name='num of y')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de7119-7f14-43e3-a83f-9824be677e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Grouping Data \n",
    "##Aggregation\n",
    ">>> df2.groupby(by=['Date','Type']).mean()\n",
    ">>> df4.groupby(level=0).sum()\n",
    ">>> df4.groupby(level=0).agg({'a':lambda x:sum(x)/len (x), 'b': np.sum})\n",
    "\n",
    "\n",
    "##DATES\n",
    ">>> df2['Date']= pd.to_datetime(df2['Date'])\n",
    ">>> df2['Date']= pd.date_range('2000-1-1',\n",
    "          periods=6, \n",
    "          freq='M')\n",
    ">>>dates= [datetime(2012,5,1), datetime(2012,5,2)]\n",
    ">>>index= pd.Datetimelndex(dates)\n",
    ">>>index= pd.date_range(datetime(2012,2,1), end, freq='BM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef1732-bd51-4954-825e-82cecd65f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(group,column,plot):\n",
    "    ax=plt.figure(figsize=(12,6))\n",
    "    df.groupby(group)[column].sum().sort_values().plot(plot)\n",
    "    \n",
    "plot('Gender','Purchase','bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5a8bf-4355-447b-9cd7-c08bdfb59100",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(12,7))\n",
    "sns.countplot(df['Age'],hue=df['Gender'])\n",
    "\n",
    "plot('Age','Purchase','bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad87eb76-bcd2-4e57-bacb-02a75e1378f8",
   "metadata": {},
   "outputs": [],
   "source": [
    " How do you construct the nodes in decision tree?\n",
    "- Can you explain boosting step by step? How does Xtgboost model work?\n",
    "- Do you know the mathematical equation of l1 and l2? What do you mean by penalizing the weight absolutely?\n",
    "- Can you go through k-means step by step? How did you decide centroids and what do you mean by convergence?\n",
    "- What is word embedding?\n",
    "- In depth question about neural network\n",
    "\n",
    "Interview\n",
    "Interview was difficult. Recruiter was nice and guided through the process. First round is a HackerRank Python test. Basic string manipulation and time series. Second is a rapid fire QA of ML basics. They want you to give textbook accurate answers.\n",
    "\n",
    "Interview Questions\n",
    "Logistic + linear regression, p-value, AUC/ROC, SVM, K-means, Random forests\n",
    "\n",
    "I receive an e-mail from an external platform called HackerRank for the coding assessment. It was two programming and one modeling question. I had in total 120 minutes.\n",
    "The test was focused on the knowledge of statistics, python and its data science packages, as well as data exploration concepts.\n",
    "\n",
    "QuantHub Test\n",
    "This test is focused on your knowledge of statistics, python and its data science packages, as well as data exploration concepts.\n",
    "\n",
    "Interview Questions\n",
    "different type of charts in python\n",
    "echnical Interview Questions: What are the assumptions for Linear Regression? What is a Random Forest? How is a decision tree built?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c2780-9e16-4da9-afe5-38feb57d24e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chi square features \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_norm = MinMaxScaler().fit_transform(X)\n",
    "chi_selector = SelectKBest(chi2, k=num_feats)\n",
    "chi_selector.fit(X_norm, y)\n",
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "print(str(len(chi_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b06d01-61e7-4da3-a827-05c82fbebddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5)\n",
    "rfe_selector.fit(X_norm, y)\n",
    "rfe_support = rfe_selector.get_support()\n",
    "rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
    "print(str(len(rfe_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e29234-024b-4b6f-bdb5-44bc9951d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Embedded methods use algorithms that have built-in feature selection methods.\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\"), max_features=num_feats)\n",
    "embeded_lr_selector.fit(X_norm, y)\n",
    "\n",
    "embeded_lr_support = embeded_lr_selector.get_support()\n",
    "embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n",
    "print(str(len(embeded_lr_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4029fc5e-759b-4b57-ae3d-2912bc1ece2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##RandomForest to select features based on feature importance.\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats)\n",
    "embeded_rf_selector.fit(X, y)\n",
    "\n",
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
    "print(str(len(embeded_rf_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc817a4-6e16-4239-aca4-7506b5956726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The answer is sometimes it won’t be possible with a lot of data and time crunch.\n",
    "\n",
    "#But whenever possible, why not do this?\n",
    "# put all selection together\n",
    "feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n",
    "                                    'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n",
    "# count the selected times for each feature\n",
    "feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "# display the top 100\n",
    "feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "feature_selection_df.head(num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909fd01c-b73c-4408-8f5c-c21967b6ec39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b4563-48c6-41f9-8059-23e2f48f6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Review the data and understand what I am trying to predict.\n",
    "Is the variable a catergoical or numerical value? \n",
    "eg. gender, levels or height, salary\n",
    "build a classification (X quanlitative) or regression model (Y quantative)\n",
    "If catergoical are their labels? supervised \n",
    "unsupervised algorithim for non labeled x values \n",
    "\n",
    "Conduct EDA. \n",
    "tranform data into standard values that can be compared. \n",
    "encode catergoical values with from skllearn.preprocessing import LabelEncoder\n",
    "labelencoder_Y= LabelEncoder()\n",
    "Y = labelencoder_Y.fit_tranform(Y)\n",
    "\n",
    "Feature scaling\n",
    "\n",
    "We want to be able to explain our model and when we have too many features vs rows(observations) we won't be able to generalize our sample\n",
    "a large number of features make a model, bulky, time-consuming and harder to implement in production\n",
    "\n",
    "we need to conduct a filter to find the best features that affect our dependent variable. \n",
    " we can use \n",
    "1.     Filter based: We specify some metric and based on that filter features. An example of such a metric could be correlation/chi-square.\n",
    "2. Wrapper-based: Wrapper methods consider the selection of a set of features as a search problem. Example: Recursive Feature Elimination\n",
    "3. Embedded: Embedded methods use algorithms that have built-in feature selection methods. For instance, Lasso and RF have their own feature selection methods.\n",
    "\n",
    "normalize/standardize depending on the model\n",
    "fill in nan values\n",
    "After conducting data preprocessing we split the data split the data into 80-20/60-20-20/ or cross validation with 5 or 10 folds \n",
    "\n",
    "depending on the data and what we want to find we apply a supervised or unsupervised model\n",
    "regression/classification = both input and output data are provided. \n",
    "\n",
    "unsupervised = using info that is not classified or lableled. \n",
    "labels/no labels\n",
    "\n",
    "\n",
    "Cat. data\n",
    "encode catergoical values with from skllearn.preprocessing import LabelEncoder\n",
    "labelencoder_Y= LabelEncoder()\n",
    "Y = labelencoder_Y.fit_tranform(Y)\n",
    "\n",
    "Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Modelselection \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
